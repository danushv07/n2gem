{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook contains the additional analysis part of the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To use any part of this notebook, load the ```model dataset``` and ```validation set```\n",
    "- Perform the FGSM attack using the validation set and follow the foll. procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import os \n",
    "os.environ[\"GIT_PYTHON_REFRESH\"] = \"quiet\" \n",
    "#!module load git\n",
    "import foolbox as fb\n",
    "import torch\n",
    "import eagerpy as ep\n",
    "from foolbox import PyTorchModel, accuracy, samples\n",
    "import numpy as np\n",
    "from n2gem.metrics import gem_build_coverage, gem_build_density\n",
    "from n2gem.aux_funcs import gem_build_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from util_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from fastai.vision.all import *\n",
    "from fastai.vision import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import medmnist\n",
    "from medmnist import INFO, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix the seed generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42) \n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#print(device); #print(torch.cuda.memory_allocated())\n",
    "#torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run these cells before analysis\n",
    "\n",
    "Only the example for MNIST dataset is showed here. Follow the same steps as in ```adversarial_attack_``` to perform the same for all the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the respective models for the attack\n",
    "MyModel = MnNet() # MnNet, PathNet, OrgNet\n",
    "\n",
    "# mnist\n",
    "MyModel.load_state_dict(torch.load('chkpt_files/fastai_MnNet_weights.pth', map_location=device))\n",
    "\n",
    "MyModel.eval()\n",
    "#print(MyModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Pytorch model for foolbox attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist\n",
    "preprocess = dict(mean=0.1307, std=0.3081)\n",
    "\n",
    "bound = (0, 1)\n",
    "original_model = fb.PyTorchModel(MyModel, bounds=bound, preprocessing=preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Attack with 20 values of epsilons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack2 = fb.attacks.FGSM()\n",
    "epsilon = np.linspace(0.0, 1, num=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_images = torch.load('images/model_dataset_images.pt', map_location='cpu').cpu()\n",
    "md_labels = torch.load('images/model_dataset_labels.pt', map_location='cpu').cpu()\n",
    "\n",
    "vali_images = torch.load('images/validation_images.pt', map_location='cpu')\n",
    "vali_labels = torch.load('images/validation_labels.pt', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the model_dataset to obtain 20000 images for the attack\n",
    "\n",
    "# mnist\n",
    "_, X_images, _, y_labels = train_test_split(md_images.numpy(), md_labels.numpy(), test_size=0.29455, random_state=42, stratify=md_labels.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape and form the model_dataset tensors--> named as images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist\n",
    "images = ep.astensor(torch.from_numpy(np.array(X_images)).to(device))\n",
    "\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images from the validation set\n",
    "# resize the real images\n",
    "real = images.raw.view(images.shape[0], -1)\n",
    "#real.shape, type(real)\n",
    "\n",
    "gen_validate = torch.from_numpy(np.array(vali_images).reshape(len(vali_images), -1)).to(device)\n",
    "gen_labels = torch.from_numpy(np.array(vali_labels).reshape(-1)).to(device)\n",
    "print(gen_validate.shape)\n",
    "\n",
    "\n",
    "density_validate = gem_build_density(real, real.shape[0], gen_validate, 'indexflatl2')\n",
    "coverage_validate = gem_build_coverage(real, real.shape[0], gen_validate, 'indexflatl2')\n",
    "print(f'density: {density_validate:.5f}, coverage: {coverage_validate:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attack the model using validation dataset\n",
    "\n",
    "- convert the validation_images/labels into ep.tensor for foolbox attack compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vali_imagesx = ep.astensor(torch.from_numpy(np.array(vali_images)).to(device))\n",
    "vali_labelsx = ep.astensor(torch.from_numpy(np.array(vali_labels).reshape(-1)).to(device))\n",
    "print(vali_imagesx.shape, vali_labelsx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils import model_attack\n",
    "adv_vali, adv_info_vali = model_attack(attack2, original_model, vali_imagesx, vali_labelsx, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pymde visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the reduced dimensionality plots of the adv. samples\n",
    "\n",
    "pred = []\n",
    "emb = []\n",
    "epi = [0,0.25,0.5,0.75,1] # specific epsilon values\n",
    "t = [0,5,10,15,-1] # specfic adv. samples \n",
    "for i in tqdm(t): \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image = adv_vali[i].raw.cpu().to(device)\n",
    "        predict = MyModel(image)\n",
    "        predictions = np.argmax(predict.cpu().numpy(), axis=1)\n",
    "    \n",
    "    # single epsilon adv images\n",
    "    img = adv_vali[i].raw.view(adv_vali[i].shape[0], -1)\n",
    "    \n",
    "    embdd = pymde.preserve_neighbors(img, embedding_dim=2, constraint=pymde.Standardized()).embed() \n",
    "    \n",
    "    pred.append(predictions)\n",
    "    emb.append(embdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pymde plots (code has been adapted from this part of the pymde repo- plot section(https://github.com/cvxgrp/pymde/blob/main/pymde/problem.py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,5, figsize=(14,7))\n",
    "fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "c = 0\n",
    "ticks = np.arange(0,10,1).tolist()\n",
    "a = [16,17,18,19,20]\n",
    "episo = np.arange(0, 1.05, 0.05)\n",
    "episo = np.delete(episo, 18)\n",
    "xtick = [-3,0,3]\n",
    "k = 0\n",
    "images = []\n",
    "for i in range(5):\n",
    "    im = ax[i].scatter(emb[k][:,0], emb[k][:,1], c=pred[k], s=1.0, cmap=\"Spectral\")\n",
    "\n",
    "    ax[i].label_outer()\n",
    "    #ax[i].set_aspect('equal', adjustable='datalim')\n",
    "    #ax[i,j].set_aspect(\"equal\", adjustable=\"box\")\n",
    "    ax[i].set_facecolor('gray')\n",
    "    ax[i].set_title((r'$\\epsilon$' + f\"={epi[k]:.2f}\"))\n",
    "   \n",
    "\n",
    "    lim_low = min(np.min(emb[k][:,0].numpy()), np.min(emb[k][:,1].numpy())) * 1.1\n",
    "    lim_high = max(np.max(emb[k][:,0].numpy()), np.max(emb[k][:,1].numpy())) * 1.1\n",
    "    lim = _square_lim(lim_low, lim_high)\n",
    "    ax[i].set_xlim(lim)\n",
    "    ax[i].set_ylim(lim)\n",
    "    ax[i].set_aspect(\"equal\", adjustable=\"box\")\n",
    "    k+= 1\n",
    "#aspect = 30\n",
    "#divider = make_axes_locatable(im)\n",
    "#cax = divider.append_axes(\"right\", size=\"3.%\", pad=0.05)\n",
    "\n",
    "cbar = fig.colorbar(im, ax=ax, location='bottom', boundaries=np.arange(11) - 0.5, pad=0.1, fraction=.1, shrink=0.5) #orientation='vertical'\n",
    "cbar.set_ticks(np.arange(11).tolist())\n",
    "#cbar.ax.set_yticklabels(ticks)\n",
    "plt.savefig('mnist_pymde.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The realtive change in cluster volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a faiss tree on the validation images and query the adv. samples\n",
    "\n",
    "real_vali = vali_imagesx.raw.view(vali_imagesx.shape[0], -1)\n",
    "real_vali.shape\n",
    "vali_tree = gem_build_tree(real_vali, real_vali.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vali_tree = gem_build_tree(real_vali, real_vali.shape[0])\n",
    "cluster = []; value =[]\n",
    "\n",
    "for j in range(len(epsilon)):\n",
    "    cluster = []\n",
    "    with torch.no_grad():\n",
    "        image = adv_vali[j]\n",
    "        predict = MyModel(image)\n",
    "        predictions = np.argmax(predict.cpu().numpy(), axis=1)\n",
    "    for i in range(9):\n",
    "        idx = np.where(predictions==i)[0]\n",
    "        if(len(idx) == 0):\n",
    "            real_radii = 0\n",
    "            cluster.append(real_radii)\n",
    "        else:\n",
    "            query_imgs = adv_vali[j][idx].reshape(idx.shape[0], -1)\n",
    "            D, I = vali_tree.search(query_imgs, 25)\n",
    "            #print(la.matrix_norm(D))\n",
    "            #real_radii = torch.max(torch.sqrt(D), dim=1)[0].mean()\n",
    "            #real_radii = (torch.sqrt(D).mean(dim=1)).mean()\n",
    "            real_radii = ((torch.sqrt(D).max(dim=1))[0]**3 * np.pi * (4.0/3.0)).sum()\n",
    "            cluster.append(real_radii.cpu())\n",
    "    value.append(cluster)\n",
    "#cluster = np.array(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the relative change in volume between each pred. class clusters\n",
    "clust = []\n",
    "for i in range(len(value)):\n",
    "    a = value[i].reshape(value[i].shape[0], -1)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        s = abs(a - a[:,None])\n",
    "        idx = np.isnan(s)\n",
    "        idxx = np.isinf(s)\n",
    "        s[idx] = 0\n",
    "        s[idxx] = 0\n",
    "        mat = np.linalg.norm(s, axis=-1)\n",
    "    clust.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colors\n",
    "fig, ax = plt.subplots(1,5, sharex=True, sharey=True, figsize=(10,10))\n",
    "fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "c = 0\n",
    "ticks = np.arange(0,9,1).tolist()\n",
    "a = [16,17,18,19,20]\n",
    "epi = [0,0.25,0.5,0.75,1]\n",
    "episo = np.arange(0, 1.05, 0.05)\n",
    "episo = np.delete(episo, 18)\n",
    "f = [0,5,10,15,19]\n",
    "\n",
    "images = []\n",
    "for i,j in zip(range(5),f):\n",
    "    \n",
    "    images.append(ax[i].imshow(clust[j], cmap=plt.cm.magma))\n",
    "    ax[i].label_outer()\n",
    "    ax[i].set_aspect('equal')\n",
    "    \n",
    "    ax[i].set_xticks(ticks)\n",
    "    ax[i].set_yticks(ticks) \n",
    "    ax[i].set_title((r'$\\epsilon$' + f\"={epi[i]:.2f}\"))\n",
    "#vmin = min(image.get_array().min() for image in images)\n",
    "#vmax = max(image.get_array().max() for image in images)\n",
    "#norm = colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "#for im in images:\n",
    "#    im.set_norm(norm)\n",
    "\n",
    "fig.colorbar(images[0], ax=ax,location='bottom', pad=0.05, fraction=.1, shrink=0.5) #orientation='vertical'\n",
    "\n",
    "#cbar = fig.colorbar(images[0], ax=ax, location='bottom', boundaries=np.arange(11) - 0.5, pad=0.1, fraction=.1, shrink=0.5) #orientation='vertical'\n",
    "#cbar.set_ticks(np.arange(11).tolist())\n",
    "\n",
    "#for im in images:\n",
    "#    im.callbacks.connect('changed', update)\n",
    "#fig.tight_layout()\n",
    "#plt.show()\n",
    "#plt.savefig('mnist_vol_norm.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time analysis between ```prdc``` & ```n2gem```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_features = []\n",
    "fake_features = []\n",
    "samples = [1000, 5000, 10000, 25000, 50000]\n",
    "for i in tqdm(samples):\n",
    "    real_features.append(np.random.normal(loc=0.0, scale=1.0,\n",
    "                                 size=[i, feature_dim]))\n",
    "    fake_features.append(np.random.normal(loc=0.0, scale=1.0,\n",
    "                                 size=[i, feature_dim]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(samples)):\n",
    "    s_time = time.time()\n",
    "    metrics = compute_prdc(real_features=real_features[i],\n",
    "                       fake_features=fake_features[i],\n",
    "                       nearest_k=nearest_k)\n",
    "    e_time = time.time()\n",
    "    print(\"time: \", e_time - s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(samples)):\n",
    "    s_time = time.time()\n",
    "    den = gem_build_density(real_features[i], real_features[i].shape[0], fake_features[i], 'indexflatl2')\n",
    "    cov = gem_build_coverage(real_features[i], real_features[i].shape[0], fake_features[i], 'indexflatl2')\n",
    "    e_time = time.time()\n",
    "    print(\"time: \", e_time - s_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "pr = <endter the prdc time value>     #[0.777, 2.116, 6.903, 35.56, 150.285]\n",
    "n2cpu = <enter the cpu time value of n2gem prdc> # [0.29, 0.633, 0.752, 4.11, 13.33]\n",
    "n2gpu = <enter the gpu time value of n2gem prdc> # [0.22, 0.33, 0.38, 0.66,1.465]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "ax[0].plot(samples, pr, c='b', ls='-.', marker='o', label='prdc')\n",
    "ax[0].set_xlabel(\"sample size\")\n",
    "ax[0].set_ylabel(\"time(s)\")\n",
    "ax[0].set_title(\"prdc\", fontweight=\"bold\")\n",
    "ax[0].legend()\n",
    "ax[1].plot(samples, n2cpu, c='g', ls='-.', marker='+',label='cpu_run')\n",
    "ax[1].plot(samples, n2gpu, c='r', ls='--', marker='+',label='gpu_run')\n",
    "ax[1].set_xlabel(\"sample size\")\n",
    "ax[1].set_ylabel(\"time(s)\")\n",
    "ax[1].set_title(\"Our implementation\", fontweight=\"bold\")\n",
    "ax[1].legend()\n",
    "plt.savefig('time_analysis.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for FGSM Attack - stats\n",
    "\n",
    "- to use the following cells, the dataset needs to be loaded and the attack has to be performed\n",
    "\n",
    "\n",
    "- for the computation of confidence bands\n",
    "- ```Batch_modelvali_metrics``` - density & coverage between model_dataset& validation_dataset in batches of images\n",
    "- ```Batch_modelvali_adv_metrics``` - attack the model in batches using validation_dataset. Computed density & coverage between model_dataset & validation_adv samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Load the images directly from Attack the model section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist\n",
    "#realx = torch.from_numpy(np.array(X_images)).view(len(X_images), -1).to(device)\n",
    "\n",
    "# pathmnist\n",
    "#realx = md_images.reshape(md_images.shape[0], -1).to(device)\n",
    "vali_attk_images = torch.from_numpy(np.array(vali_images)).to(device)\n",
    "vali_attk_labels = torch.from_numpy(np.array(vali_labels).reshape(-1)).to(device)\n",
    "epsilon = np.linspace(0.0, 1, num=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realx.shape, vali_attk_images.shape, vali_attk_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch_modelvali_metrics = model_validation_metrics_batches(realx, vali_attk_images, vali_attk_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch_modelvali_adv_metrics = batch_validation_attack(real, vali_attk_images, vali_attk_labels, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "with open(<f_name>, 'w') as newfile:\n",
    "    newfile.write(\"# FGSM attack with model_dataset(30547 images)\" + \"\\n\" +\n",
    "                 \"# and validation_images(5359) 20 epsilon in batches of 100 images\" + \"\\n\" +\n",
    "                 \"# metrics between model_dataset & validation dataset\" + \"\\n\" +\n",
    "                 \"# Model_vali_density Model_vali_coverage\" + \"\\n\" )\n",
    "with open('FGSM_attack_pathmnist_1C_100batch_metrics.dat', 'a') as newfile:\n",
    "    np.savetxt(newfile, np.array(Batch_modelvali_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(<f_name>, 'w') as newfile:\n",
    "    newfile.write(\"# FGSM attack with model_dataset(30547 images)\" + \"\\n\" +\n",
    "                 \"# and validation_images(5359) 20 epsilon in batches of 100 images\" + \"\\n\" +\n",
    "                 \"# metrics between model_dataset & adv validation dataset\" + \"\\n\" +\n",
    "                 \"# Model_vali_adv_density Model_vali_adv_coverage\" + \"\\n\" )\n",
    "with open('FGSM_attack_pathmnist_1C_100batch_adv_metrics.dat', 'a') as newfile:\n",
    "    np.savetxt(newfile, np.array(Batch_modelvali_adv_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for Boundary Attack - stats\n",
    "### Attack the model with validation_dataset in batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create the adversarial samples for each batch exclusively\n",
    "- use these images as starting_points for boundary attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the adversarial starting points for each Batch\n",
    "- load the 42Batch_Vali_Adv_LSBU.pt file for the saved LSBU adversarials( batch size of 42 images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = np.arange(0, <vali_sample_size>, 100)\n",
    "success = []\n",
    "batch_adv_samples = []\n",
    "for i in range(len(batches)-1):\n",
    "    start = batches[i]; end= batches[i+1]\n",
    "    _, batch_adv_lsbu, adv_bdy_info = n_attack(original_model, vali_attk_images[start:end,...], vali_attk_labels[start:end], epsilons=None)\n",
    "    print(f\"acc: {1 - adv_bdy_info.float32().mean(axis=-1)}\")\n",
    "    success.append(adv_bdy_info); batch_adv_samples.append(batch_adv_lsbu.raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_adv_lsbu_samples = torch.stack(batch_adv_samples)\n",
    "torch.save(batch_adv_lsbu_samples, '140Batch_Vali_Adv_LSBU.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_lsbu_adv = torch.load('140Batch_Vali_Adv_LSBU.pt', map_location='cpu').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoundaryAttack on the model using batches of validation_dataset\n",
    "\n",
    "- form batches of validation_images\n",
    "- perform attck using these images\n",
    "- various batch_sizes:50, 100, 150, 200, 250, 300 (dependent on the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 53, 100, 150, 200, 250, 300 \n",
    "batches = np.arange(0, 4707, 49)\n",
    "batch_bdy_adv = []\n",
    "batch_bdy_info = []\n",
    "for i in range(len(batches)-1):\n",
    "    start = batches[i]; end= batches[i+1]\n",
    "    _, adv_bdy, adv_bdy_info = BdyAttack(original_model, vali_attk_images[start:end,...], vali_attk_labels[start:end], \n",
    "                                          epsilons=None) #starting_points=batch_lsbu_adv[i],\n",
    "    batch_bdy_adv.append(adv_bdy); batch_bdy_info.append(adv_bdy_info)\n",
    "    if(i%10==0):print(f\"batch{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### model_dataset & validation_dataset in batches\n",
    "- #### model_deataset & validation adv in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_density_validate = []\n",
    "batch_coverage_validate = []\n",
    "batch_model_density = []\n",
    "batch_model_coverage = []\n",
    "real = real.to(device)\n",
    "\n",
    "for i in range(len(batches)-1):\n",
    "    \n",
    "    # reference metric\n",
    "    # model_dataset & validation_dataset in batches\n",
    "    start = batches[i]; end= batches[i+1]\n",
    "    bth_val_imgs = vali_attk_images[start:end,...]\n",
    "    gen_validatex = bth_val_imgs.raw.view(bth_val_imgs.shape[0], -1)\n",
    "    gen_labelsx = vali_attk_labels[start:end].raw\n",
    "    density_validatex = gem_build_density(real, real.shape[0], gen_validatex, 'indexflatl2')\n",
    "    coverage_validatex = gem_build_coverage(real, real.shape[0], gen_validatex, 'indexflatl2')\n",
    "    \n",
    "    # adversarial metric\n",
    "    # model_dataset & validation_dataset adversarials in batches\n",
    "    gen_adv_valx = batch_bdy_adv[i].raw.view(batch_bdy_adv[i].shape[0], -1)\n",
    "    model_density_valx = gem_build_density(real, real.shape[0], gen_adv_valx, 'indexflatl2')\n",
    "    model_coverage_valx = gem_build_coverage(real, real.shape[0], gen_adv_valx, 'indexflatl2')\n",
    "    \n",
    "    batch_density_validate.append(density_validatex)\n",
    "    batch_coverage_validate.append(coverage_validatex)\n",
    "    batch_model_density.append(model_density_valx)\n",
    "    batch_model_coverage.append(model_coverage_valx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save metrics to file\n",
    "\n",
    "- info. in each file\n",
    "    - attack name with dataset info.\n",
    "    - density metric btw. model_dataset & validation_Set\n",
    "    - density metric btw. model_dataset & adv. samples\n",
    "    - coverage metric btw. model_dataset & validation_Set\n",
    "    - coverage metric btw. model_dataset & adv. samples\n",
    "    - Columns: Batch_number | model_validation_densiy | model_adv_density | model_validation_coverage\n",
    "        model_adv_coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Boundary_attack_organmnist_100batch_metrics.dat', 'w') as newfile:\n",
    "    newfile.write(\"# Boundary attack with validation_dataset(4708 images)\" + \"\\n\" +\n",
    "                 \"# and metrics using model_dataset(54142)\" + \"\\n\" +\n",
    "                 \"# Model_dataset & validation set: density: \" + str(density_validate.cpu().numpy()) + \"\\n\" +\n",
    "                 \"# Model_dataset & validation adv (4708): density: \" + str(model_density_val.cpu().numpy()) + \"\\n\" +\n",
    "                 \"# Model_dataset & validation set: coverage: \" + str(coverage_validate.cpu().numpy()) + \"\\n\" +\n",
    "                 \"# Model_dataset & validation adv (4708): coverage: \" + str(model_coverage_val.cpu().numpy()) + \"\\n\" + \n",
    "                 \"# batch_no model_val_density model_val_adv_density model_val_coverage model_val_adv_coverage\" + \"\\n\")\n",
    "    \n",
    "with open('Boundary_attack_organmnist_100batch_metrics.dat', 'a') as nefile:\n",
    "    for i in range (len(batch_density_validate)):\n",
    "        nefile.write(str(i)+ \" \" + str(batch_density_validate[i].cpu().numpy())+ \" \" + str(batch_model_density[i].cpu().numpy())+ \" \" +\n",
    "                    str(batch_coverage_validate[i].cpu().numpy()) + \" \" + str(batch_model_coverage[i].cpu().numpy()) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------\n",
    "-------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following results are not included in the paper and can be avoided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PYMDE visualizations of feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The features from the intermediate layers were extracted and were visualized using pymde\n",
    "- These results are not included in the paper and can be omitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pymde plot for model_dataset images( both model and FX_model)\n",
    "dimPlot(real_FX, y_labels, None)\n",
    "plt.tight_layout()\n",
    "plt.savefig('pathmnist_pymde/FXmodel.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimPlot(real_FX, np.array(y_labels),None)\n",
    "plt.tight_layout()\n",
    "plt.savefig('mnist_pymde/FXmodel_20000.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_predictions = []\n",
    "for i in range(len(epsilon)):\n",
    "    with torch.no_grad():\n",
    "        image = adv_vali[i].raw.cpu()\n",
    "        predict = MyModel(image)\n",
    "        predictions = np.argmax(predict.cpu().numpy(), axis=1)\n",
    "    #adv_predictions.append(predictions)\n",
    "    dimPlot(adv_vali[i].raw.reshape(adv_vali[i].shape[0], -1), predictions,None)# ('orgmnist_adv_epi_'+str(i)+'.png'))\n",
    "    plt.title(f'epi={(epsilon[i]):.3f}', loc='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pathmnist_pymde/'+str(i)+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pymde for adv. samples\n",
    "\n",
    "- predict the labels for generated adv. samples\n",
    "- extract features for each epsilon of adv. samples\n",
    "- pymde plot the featured extracted adv. samples with predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(epsilon)): \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image = adv_vali[i].raw.cpu().to(device)\n",
    "        predict = MyModel(image)\n",
    "        predictions = np.argmax(predict.cpu().numpy(), axis=1)\n",
    "    \n",
    "    # single epsilon adv images\n",
    "    adv_img = adv_vali[i].raw\n",
    "    \n",
    "    # get the feature vector from adv. samples\n",
    "    gen = torch.from_numpy(feature_extractor(adv_img, FXMyModel, None))\n",
    "    \n",
    "    dimPlot(gen, predictions, None)# ('orgmnist_adv_epi_'+str(i)+'.png'))\n",
    "    plt.title(f'epi={(epsilon[i]):.3f}', loc='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pathmnist_pymde/FXadv_pred'+str(i)+'.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-n2gem",
   "language": "python",
   "name": "n2gem"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
